{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c41f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5dccd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_classifier = pipeline(\"sentiment-analysis\")\n",
    "result = sentiment_classifier(\"I love using Hugging Face Transformers!\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4c5e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_pipeline = pipeline(\"ner\", model=\"dslim/bert-base-NER\")\n",
    "text = \"Hugging Face is creating a tool that democratizes AI.\"\n",
    "ner_results = ner_pipeline(text)\n",
    "for entity in ner_results:\n",
    "    print(f\"Entity: {entity['word']}, Label: {entity['entity']}, Score: {entity['score']:.2f}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c26960",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_shot_classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "sequence = \"Hugging Face is creating a tool that democratizes AI.\"\n",
    "candidate_labels = [\"technology\", \"education\", \"politics\"]\n",
    "zero_shot_result = zero_shot_classifier(sequence, candidate_labels)\n",
    "print(zero_shot_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d9fe3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "model = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "text = \"Hugging Face is creating a tool that democratizes AI.\"\n",
    "input_ids = tokenizer(text)\n",
    "print(input_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305dbe83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the text into tokens\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(tokens)\n",
    "# convert tokens to token IDs\n",
    "toekn_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(toekn_ids)\n",
    "# convert token IDs back to tokens\n",
    "tokens_back = tokenizer.convert_ids_to_tokens(toekn_ids)\n",
    "print(tokens_back)\n",
    "# convert token IDs to text\n",
    "text_back = tokenizer.decode(toekn_ids)\n",
    "print(text_back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192166e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# there is a difference between when we call tokenizer(text) and tokenizer(tokenized_text) and then tokenizer.convert_tokens_to_ids()\n",
    "# in the first casse tehre are special tokens added, in the second case there are not\n",
    "# let's see the difference\n",
    "tokenized_text = tokenizer.tokenize(text)\n",
    "print(tokenized_text)\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "print(token_ids)\n",
    "token_ids_with_special_tokens = tokenizer(text)\n",
    "print(token_ids_with_special_tokens)\n",
    "# print the token with ID 101\n",
    "print(tokenizer.convert_ids_to_tokens([101]))\n",
    "# print the token with ID 102\n",
    "print(tokenizer.convert_ids_to_tokens([102]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a69169",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
