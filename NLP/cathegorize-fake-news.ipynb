{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb5ca1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy import tokenizer\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.models import LsiModel, TfidfModel\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2a6904",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b624e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set plot options\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "default_plot_colour =\"#00bfbf\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02001ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('fake_news_data.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be484e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b28dbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the number of articles in each category\n",
    "data['fake_or_factual'].value_counts().plot(kind='bar', color=default_plot_colour)\n",
    "plt.title('Number of Articles in Each Category')\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Number of Articles')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f72e4b",
   "metadata": {},
   "source": [
    "# POS Tagging\n",
    "\n",
    "**Part-of-Speech (POS) tagging** is the process of labeling each word in a text with its grammatical role, such as noun, verb, adjective, etc. POS tagging helps computers understand the structure and meaning of sentences by identifying how words function in context.\n",
    "\n",
    "**Common POS tags include:**\n",
    "- Noun (N): person, place, thing (e.g., \"dog\", \"city\")\n",
    "- Verb (V): action or state (e.g., \"run\", \"is\")\n",
    "- Adjective (ADJ): describes a noun (e.g., \"happy\", \"blue\")\n",
    "- Adverb (ADV): modifies a verb, adjective, or adverb (e.g., \"quickly\", \"very\")\n",
    "- Pronoun (PRON): replaces a noun (e.g., \"he\", \"they\")\n",
    "- Preposition (PREP): shows relationship (e.g., \"in\", \"on\")\n",
    "- Conjunction (CONJ): connects words or phrases (e.g., \"and\", \"but\")\n",
    "- Determiner (DET): specifies a noun (e.g., \"the\", \"some\")\n",
    "\n",
    "**Why is POS tagging important?**\n",
    "- Enables deeper text analysis and understanding\n",
    "- Useful for information extraction, named entity recognition, and syntactic parsing\n",
    "- Helps improve the accuracy of downstream NLP tasks like sentiment analysis and machine translation\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a95a765",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.cli.download(\"en_core_web_sm\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12dcdd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_news = data[data['fake_or_factual'] == 'Fake News']\n",
    "fake_news.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5937e8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "factual_news = data[data['fake_or_factual'] == 'Factual News']\n",
    "factual_news.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12e2a8d",
   "metadata": {},
   "source": [
    "This approach is much faster than looping over each text with `nlp(text)` because `nlp.pipe` processes texts in batches and is optimized for large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24695fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fake_spacydocs = [nlp(text) for text in fake_news['text']]\n",
    "fake_spacydocs = list( nlp.pipe(fake_news['text']))\n",
    "factual_spacydocs = list( nlp.pipe(factual_news['text']))\n",
    "# print information about the first document\n",
    "print(fake_spacydocs[0].text)\n",
    "print(fake_spacydocs[0].ents)\n",
    "print(fake_spacydocs[0].ents[0].label_)\n",
    "print(fake_spacydocs[0].ents[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99395ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_token_tags ( doc: spacy.tokens.doc.Doc):\n",
    "    \"\"\"\n",
    "    Extracts token text and part-of-speech tags from a spaCy Doc object.\n",
    "    \n",
    "    Args:\n",
    "        doc (spacy.tokens.doc.Doc): The spaCy Doc object to extract from.\n",
    "        \n",
    "    Returns:\n",
    "        list: A list of tuples containing token text and its part-of-speech tag.\n",
    "    \"\"\"\n",
    "    return [(token.text, token.ent_type_ ,token.pos_) for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e796c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_tags_df = []\n",
    "columns = ['token', 'ner_tag', 'pos_tag']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07738777",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for doc in fake_spacydocs:\n",
    "    tags = extract_token_tags(doc)\n",
    "    tags = pd.DataFrame(tags, columns=columns)\n",
    "    fake_tags_df.append(tags)\n",
    "fake_tags_df = pd.concat(fake_tags_df, ignore_index=True)\n",
    "fake_tags_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1a5d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "factual_tags_df = []\n",
    "# the columns are the same as for the fake news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e33e016",
   "metadata": {},
   "outputs": [],
   "source": [
    "factual_tags_df = [ pd.DataFrame(extract_token_tags(doc), columns=columns) for doc in factual_spacydocs ]\n",
    "factual_tags_df = pd.concat(factual_tags_df, ignore_index=True)\n",
    "factual_tags_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84bd595",
   "metadata": {},
   "source": [
    "# POS Frequency Analysis\n",
    "\n",
    "This section analyzes the frequency of different part-of-speech tags in both fake and factual news articles. By comparing POS tag distributions, we can identify linguistic patterns that may distinguish fake news from factual reporting.\n",
    "\n",
    "**What this analysis does:**\n",
    "- Counts how often each grammatical category (NOUN, VERB, ADJ, etc.) appears in fake vs factual news\n",
    "- Groups tokens by their part-of-speech tags to find the most common word types\n",
    "- Compares linguistic patterns between fake and factual news articles\n",
    "- Provides quantitative evidence of grammatical differences in news types\n",
    "\n",
    "**Key insights:**\n",
    "- Shows which grammatical categories (nouns, verbs, adjectives, etc.) are most common in each news type\n",
    "- Helps identify potential linguistic markers for fake news detection\n",
    "- Provides foundation for feature engineering in classification models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ade3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_counts_fake = fake_tags_df \\\n",
    "    .groupby(['token', 'pos_tag']) \\\n",
    "    .size() \\\n",
    "    .reset_index(name='counts') \\\n",
    "    .sort_values(by='counts', ascending=False)\n",
    "# display the top 10 most common tokens and their part-of-speech tags\n",
    "pos_counts_fake.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d72a2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_counts_factual = factual_tags_df \\\n",
    "    .groupby(['token', 'pos_tag']) \\\n",
    "    .size() \\\n",
    "    .reset_index(name='counts') \\\n",
    "    .sort_values(by='counts', ascending=False)\n",
    "# display the top 10 most common tokens and their part-of-speech tags\n",
    "pos_counts_factual.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7560fff",
   "metadata": {},
   "source": [
    "## POS Tag Frequency Analysis\n",
    "\n",
    "This code analyzes the diversity of vocabulary within each part-of-speech category in fake news articles:\n",
    "\n",
    "**What it does:**\n",
    "- Groups the data by POS tags (NOUN, VERB, ADJ, etc.)\n",
    "- Counts how many **unique token types** exist for each POS tag\n",
    "- Sorts results in descending order to show categories with the most vocabulary diversity\n",
    "- Displays the top 10 POS tags with the highest token variety\n",
    "\n",
    "**What the results tell us:**\n",
    "- Which grammatical categories have the richest vocabulary in fake news\n",
    "- The linguistic complexity and diversity of different word types\n",
    "- Potential indicators of writing style differences between fake and factual news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68cb449",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_counts_fake.groupby(['pos_tag'])['token'] \\\n",
    "    .count() \\\n",
    "    .sort_values(ascending=False)[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1874ad67",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_counts_factual.groupby(['pos_tag'])['token'] \\\n",
    "    .count() \\\n",
    "    .sort_values(ascending=False)[:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0acbab",
   "metadata": {},
   "source": [
    "## Most Common Nouns Analysis\n",
    "\n",
    "This analysis filters the data to show only NOUN tokens and identifies the most frequently used nouns in fake news articles. By examining the top 10 most common nouns, we can understand what topics and entities are most prominently discussed in fake news content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236b0331",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_counts_fake[pos_counts_fake['pos_tag'] == 'NOUN'].\\\n",
    "    sort_values(by='counts', ascending=False)[:10]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20c4909",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_counts_factual[pos_counts_factual['pos_tag'] == 'NOUN'].\\\n",
    "    sort_values(by='counts', ascending=False)[:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87945f3f",
   "metadata": {},
   "source": [
    "# Named Entities \n",
    "This section analyzes named entities in the dataset, focusing on how often different types of entities (like PERSON, ORGANIZATION, GPE, etc.) appear in fake vs factual news articles. Named entity recognition (NER) helps identify specific people, organizations, locations, and other important entities mentioned in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40341a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_tags_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d864db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the most common named entities in fake news\n",
    "fake_ner_counts = fake_tags_df[fake_tags_df['ner_tag'] != ''] \\\n",
    "    .groupby(['ner_tag', 'token']) \\\n",
    "    .size() \\\n",
    "    .reset_index(name='counts') \\\n",
    "    .sort_values(by='counts', ascending=False)\n",
    "fake_ner_counts.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18c37be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the most common named entities in factual news\n",
    "factual_ner_counts = factual_tags_df[factual_tags_df['ner_tag'] != ''] \\\n",
    "    .groupby(['ner_tag', 'token']) \\\n",
    "    .size() \\\n",
    "    .reset_index(name='counts') \\\n",
    "    .sort_values(by='counts', ascending=False)\n",
    "factual_ner_counts.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d806bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare a palette for the named entity visualisation in seaborn\n",
    "sns.barplot(\n",
    "            x='counts',\n",
    "            y='token',\n",
    "            hue='ner_tag',\n",
    "            data=fake_ner_counts[:15],\n",
    "            dodge=True,\n",
    "            orient='horizontal',\n",
    "            palette='Set2'\n",
    "    ).set_title('Most Common Named Entities in Fake News')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840c9fa6",
   "metadata": {},
   "source": [
    "# Cleanup the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac495817",
   "metadata": {},
   "outputs": [],
   "source": [
    "factual_news.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01018fff",
   "metadata": {},
   "source": [
    "remove the leading name of the news agency and the name of the city. Basically everthing which starts with text and dash and space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c29474",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text clean'] = data['text'].apply(lambda x: re.sub(r'^[^-]*-\\s', '', x))\n",
    "data[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607508c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text clean'] = data['text clean'].str.lower()\n",
    "# remove punctuation\n",
    "# Remove all punctuation\n",
    "data['text clean'] = data.apply(lambda x: re.sub(r'[^\\w\\s]', '', x['text clean']), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09faacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad5c1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords\n",
    "en_stopwords = stopwords.words('english')\n",
    "data['text clean'] = data['text clean'].apply(\n",
    "    lambda x: ' '.join([word for word in x.split() if word not in en_stopwords])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4054cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e46dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the text\n",
    "data['text clean'] = data['text clean'].apply(word_tokenize)\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936fa29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatization\n",
    "lemetizer = WordNetLemmatizer()\n",
    "nltk.download('wordnet')\n",
    "data['text clean'] = data['text clean'].apply( \n",
    "    lambda tokens: [lemetizer.lemmatize(token) for token in tokens]\n",
    ")\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4b8b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the number of  tokens\n",
    "tokens_clean = sum(data['text clean'], [])\n",
    "print(f'Total number of tokens: {len(tokens_clean)}')\n",
    "tokens_clean[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d886c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams = pd.Series(nltk.ngrams(tokens_clean, 1)).value_counts().reset_index()\n",
    "unigrams.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b9c901",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = pd.Series(nltk.ngrams(tokens_clean, 2)).value_counts().reset_index()\n",
    "bigrams.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f4cf3e",
   "metadata": {},
   "source": [
    "# Sentiment Analysis of the fake and factual news articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30a414c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vader_analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ff1911",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['vader_sentiment'] = data['text'].apply(lambda x: vader_analyzer.polarity_scores(x)['compound'])\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef802c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins=[-1, -0.1, 0.1, 1 ] # define the bins for sentiment analysis\n",
    "bin_names = [ \"negative\", \"neutral\", \"pozitive\"]  # define the bin names\n",
    "data['vader_sentiment_label'] = pd.cut(data['vader_sentiment'], bins=bins, labels=bin_names)\n",
    "data.head(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e16089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the distribution of sentiment labels\n",
    "data['vader_sentiment_label'].value_counts().plot(kind='bar', color=default_plot_colour)\n",
    "plt.title('Distribution of Sentiment Labels')\n",
    "plt.xlabel('Sentiment Label')\n",
    "plt.ylabel('Number of Articles')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23b56bb",
   "metadata": {},
   "source": [
    "- plot how many pozitive are the fake and factual news articles\n",
    "- plot how many neutral are the fake and factual news articles\n",
    "- plot how many negative are the fake and factual news articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5534b267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the composition of positive, negative and neutral articles in each category\n",
    "sns.countplot(x='fake_or_factual', hue='vader_sentiment_label', data=data, palette='Set2')\n",
    "plt.title('Composition of Sentiment Labels in Each Category')\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Number of Articles')\n",
    "plt.xticks(rotation=0)\n",
    "plt.legend(title='Sentiment Label')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185bd23e",
   "metadata": {},
   "source": [
    "# Topic Modeling\n",
    "Topic modeling is a technique used to discover abstract topics within a collection of documents. It helps in understanding the underlying themes present in the text data. In this analysis, we will apply topic modeling to both fake and factual news articles to identify the main topics discussed in each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a63a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_news_text=data[data['fake_or_factual'] == 'Fake News']['text clean'].reset_index(drop=True)\n",
    "fake_news_text.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce1ac7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_fake = corpora.Dictionary(fake_news_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca1918b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_fake = [dict_fake.doc2bow(text) for text in fake_news_text]\n",
    "doc_term_fake = [dict_fake.doc2bow(text) for text in fake_news_text] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8dfd3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_scores = []\n",
    "model_list = []\n",
    "\n",
    "min_topics = 2\n",
    "max_topics = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49919691",
   "metadata": {},
   "outputs": [],
   "source": [
    "for num_topics_i in range(min_topics, max_topics+1):\n",
    "    print(f'Processing {num_topics_i} topics...')\n",
    "    model = gensim.models.LdaModel(\n",
    "        id2word=dict_fake, \n",
    "        num_topics=num_topics_i,\n",
    "        corpus=bow_fake)\n",
    "    model_list.append(model)\n",
    "    coherence_model = CoherenceModel(\n",
    "        model=model,\n",
    "        texts=fake_news_text,\n",
    "        dictionary=dict_fake,\n",
    "        coherence='c_v'\n",
    "    )\n",
    "    coherence_scores.append(coherence_model.get_coherence())     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b479dab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(min_topics, max_topics+1), coherence_scores, marker='o')\n",
    "plt.title('Coherence Scores for LSI Models')\n",
    "plt.xlabel('Number of Topics')\n",
    "plt.ylabel('Coherence Score')\n",
    "plt.xticks(range(min_topics, max_topics+1))\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad87f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics_lda = 4\n",
    "lda_model = gensim.models.LdaModel(\n",
    "    id2word=dict_fake, \n",
    "    num_topics=num_topics_lda,\n",
    "    corpus=bow_fake)\n",
    "print(lda_model.print_topics(num_words=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22972afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_corpus(doc_term_matrix):\n",
    "    \"\"\"\n",
    "    Converts a document-term matrix to a TF-IDF corpus.\n",
    "    \n",
    "    Args:\n",
    "        doc_term_matrix (list): A list of tuples representing the document-term matrix.\n",
    "        \n",
    "    Returns:\n",
    "        list: A list of tuples representing the TF-IDF corpus.\n",
    "    \"\"\"\n",
    "    tfidf_model = TfidfModel(\n",
    "        corpus = doc_term_matrix, \n",
    "        normalize=True\n",
    "        )\n",
    "    return tfidf_model[doc_term_matrix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b94ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coherence_scores(corpus, dictionary, texts, min_topics=2, max_topics=11):\n",
    "    \"\"\"\n",
    "    Computes coherence scores for LDA models with varying number of topics.\n",
    "    \n",
    "    Args:\n",
    "        corpus (list): The document-term matrix.\n",
    "        dictionary (corpora.Dictionary): The dictionary mapping of terms to IDs.\n",
    "        texts (list): The list of tokenized documents.\n",
    "        min_topics (int): Minimum number of topics to evaluate.\n",
    "        max_topics (int): Maximum number of topics to evaluate.\n",
    "        \n",
    "    Returns:\n",
    "        list: Coherence scores for each number of topics.\n",
    "    \"\"\"\n",
    "    coherence_scores = []\n",
    "    model_list = []\n",
    "\n",
    "    for num_topics_i in range(min_topics, max_topics+1):\n",
    "        print(f'Processing {num_topics_i} topics...')\n",
    "        model = LsiModel(\n",
    "            id2word=dictionary, \n",
    "            num_topics=num_topics_i,\n",
    "            corpus=corpus)\n",
    "        model_list.append(model)\n",
    "        coherence_model = CoherenceModel(\n",
    "            model=model,\n",
    "            texts=texts,\n",
    "            dictionary=dictionary,\n",
    "            coherence='c_v'\n",
    "        )\n",
    "        coherence_scores.append(coherence_model.get_coherence())\n",
    "    \n",
    "    plt.plot(range(min_topics, max_topics+1), coherence_scores, marker='o')\n",
    "    plt.title('Coherence Scores for LSI Models')\n",
    "    plt.xlabel('Number of Topics')\n",
    "    plt.ylabel('Coherence Score')\n",
    "    plt.xticks(range(min_topics, max_topics+1))\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b948acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tfidf_fake = tfidf_corpus(doc_term_fake)\n",
    "get_coherence_scores(corpus_tfidf_fake, dict_fake, fake_news_text, min_topics=2, max_topics=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7a518e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa_model = LsiModel(\n",
    "    id2word=dict_fake, \n",
    "    num_topics=3,\n",
    "    corpus=corpus_tfidf_fake\n",
    ")\n",
    "print(lsa_model.print_topics(num_words=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefa1672",
   "metadata": {},
   "source": [
    "# Custom classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4987c7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c6a15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [ ','.join(map(str, text)) for text in data['text clean'] ]\n",
    "y = data['fake_or_factual']\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369bea37",
   "metadata": {},
   "outputs": [],
   "source": [
    "countvec = CountVectorizer() # create a CountVectorizer object\n",
    "countvec_fit = countvec.fit_transform(X) # fit and transform the data as a document-term matrix\n",
    "# print the shape of the document-term matrix\n",
    "print(f'Shape of the document-term matrix: {countvec_fit.shape}')\n",
    "# show the first 10 features\n",
    "print(f'First 10 features: {countvec.get_feature_names_out()[:10]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a61d960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for modeling transfrorm the data into a DataFrame\n",
    "bag_of_words = pd.DataFrame(countvec_fit.toarray(), columns=countvec.get_feature_names_out())\n",
    "bag_of_words.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541bc40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    bag_of_words, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46e25d4",
   "metadata": {},
   "source": [
    "# Logistic Regression Classifier\n",
    "Logistic Regression is a statistical method used for binary classification tasks. In this analysis, we will build a logistic regression classifier to distinguish between fake and factual news articles based on their textual features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0962be",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(random_state=0).fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "print(f'Accuracy: {accuracy_score(y_test, y_pred)}')\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ff967d",
   "metadata": {},
   "source": [
    "# SGD Classifier\n",
    "Stochastic Gradient Descent (SGD) Classifier is a linear classifier that uses stochastic gradient descent to optimize the model. It is particularly useful for large datasets and can handle both binary and multi-class classification tasks. In this analysis, we will implement an SGD classifier to classify news articles as fake or factual based on their content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9831fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SGDClassifier(random_state=0).fit(X_train, y_train)\n",
    "y_pred_svm = svm.predict(X_test)\n",
    "print(f'Accuracy: {accuracy_score(y_test, y_pred_svm)}')\n",
    "print(classification_report(y_test, y_pred_svm))\n",
    "print(confusion_matrix(y_test, y_pred_svm))\n",
    "# create a TF-IDF vectorizer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
